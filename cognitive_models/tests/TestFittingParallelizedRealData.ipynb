{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "279ea3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cognitive_models import *\n",
    "import unittest\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Any, Dict, Tuple, List, Union, Callable\n",
    "from typing import List\n",
    "from numpy.typing import NDArray\n",
    "from ast import literal_eval\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf14d02f",
   "metadata": {},
   "source": [
    "# Fit probalistic Reward Learning Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd154e3",
   "metadata": {},
   "source": [
    "## Load all Real Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1f00c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "PathTofile = \"/media/mohammad/New Volume/DoctoralSharif/Articles/RL and MDD/git/RL-and-DD/Data/BehvioralData/Subjects_Performance.csv\"\n",
    "# Load CSV file\n",
    "df = pd.read_csv(PathTofile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b153f3e",
   "metadata": {},
   "source": [
    "## Fit to Real Data(all subjects)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46debad",
   "metadata": {},
   "source": [
    "### Create Custom Dataload for Data fitting package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3aac97b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def custom_task_loader(file_path: str,\n",
    "                      task_col: str = 'Task', subject_group:str = 'D', **kwargs) -> List[NDArray]:\n",
    "    \"\"\"\n",
    "    Custom data loader for CSV files with a Task column containing string lists of\n",
    "    [stimulus, reaction_time, choice, reward] arrays.\n",
    "    \n",
    "    Parameters:\n",
    "    - file_path (str): Path to the CSV file.\n",
    "    - subject_col (str): Column name for subject IDs (used for grouping).\n",
    "    - task_col (str): Column name for Task data (string lists of T*4 arrays).\n",
    "    - **kwargs: Additional arguments for pandas.read_csv.\n",
    "    \n",
    "    Returns:\n",
    "    - List[NDArray]: List of per-subject data arrays (shape (n_trials, 3)).\n",
    "      Each NDArray contains [stimulus, choice, reward] columns. The order of arrays\n",
    "      matches the order of subjects in the file.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path, **kwargs)\n",
    "    df = df.drop(37)\n",
    "\n",
    "    if subject_group == 'CTL':\n",
    "       df = df[df[\"BDI\"]<=7] \n",
    "    elif subject_group == \"D\":\n",
    "       df = df[df[\"BDI\"]>=13] \n",
    "\n",
    "\n",
    "\n",
    "       \n",
    "    \n",
    "    required_cols = [task_col]\n",
    "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
    "    \n",
    "    df = df.dropna(subset=required_cols)\n",
    "    if df.empty:\n",
    "        raise ValueError(\"No valid data after removing missing values\")\n",
    "    \n",
    "    behavioral_data: List[NDArray] = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        try:\n",
    "            task_data = np.array(literal_eval(row[task_col]), dtype=float)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to parse Task data for subject {row[task_col]}: {e}\")\n",
    "        \n",
    "        if task_data.ndim != 2 or task_data.shape[1] != 4:\n",
    "            raise ValueError(f\"Task data for subject {row[task_col]} must be T*4, got shape {task_data.shape}\")\n",
    "        \n",
    "        # Extract [stimulus, choice, reward] (indices 0, 2, 3)\n",
    "        data = task_data[:, [0, 2, 3]].astype(int)\n",
    "        behavioral_data.append(data)\n",
    "    \n",
    "    if not behavioral_data:\n",
    "        raise ValueError(\"No valid subject data found\")\n",
    "    \n",
    "    return behavioral_data\n",
    "\n",
    "\n",
    "len(custom_task_loader(PathTofile,'Task','CTL'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df8973a",
   "metadata": {},
   "source": [
    "### Fitting Parallelized Apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3205ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup environment\n",
    "config_path = \"cognitive_models/tasks/mdp_pl_config.json\"  # Adjust path as needed\n",
    "try:\n",
    "    CONTEXTS, TRANSITIONS = parse_mdp_config(config_path)\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(f\"Configuration file not found: {config_path}\")\n",
    "\n",
    "class StateTask(State):\n",
    "    def __init__(self, name: str=None):\n",
    "        super().__init__(name)\n",
    "class StateQ(State):\n",
    "    def __init__(self, name: str=None, Q: NDArray[np.float64]=None):\n",
    "        super().__init__(name)\n",
    "        self.Q = Q\n",
    "\n",
    "\n",
    "# Create task and agent states(task state are real states and agents states are the representation of task states in agent)\n",
    "# Here agent states has additional Q attribute containing the value of each action potential for that states.\n",
    "StatesTask = np.array([State(name=k) for k in CONTEXTS.keys()])\n",
    "task_mdp = PL(StatesTask, CONTEXTS, TRANSITIONS)\n",
    "StatesAgent = np.array([StateQ(k, np.zeros((2,))) for k in CONTEXTS.keys()])\n",
    "\n",
    "# Initialize agent and environment\n",
    "agent = QL1_RL(task_mdp, StatesAgent, alphaP=0.1, alphaN=0.1)\n",
    "env = MDPEnvironment(agent, task_mdp)\n",
    "\n",
    "# Initialize EMGuassian with custom data loader\n",
    "em = EMGuassian(\n",
    "    environment=env,\n",
    "    num_params=2,  # Assuming 2 parameters (e.g., alpha, gamma)\n",
    "    num_iteration_em=20,  # Small number for testing\n",
    "    num_iteration_gradient_descent=10000,  # Reduced for faster testing\n",
    "    learning_rate=0.01,\n",
    "    tol=1e-6,\n",
    "    data_loader=custom_task_loader)\n",
    "\n",
    "try:\n",
    "    EstimatedData = em.fit(\n",
    "            behavioral_data=PathTofile,\n",
    "            task_col=\"Task\"\n",
    "        )\n",
    "except Exception as e:\n",
    "        print(f\"Model fitting failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8bc39f",
   "metadata": {},
   "source": [
    "### Save Estimated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156b2364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# This path is just an example.\n",
    "with open(\"/media/mohammad/New Volume/DoctoralSharif/Articles/RL and MDD/FittedData/total_dp_data.pkl\", \"wb\") as file:\n",
    "    pickle.dump(EstimatedData, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcab141",
   "metadata": {},
   "source": [
    "## Fit Availabel Subjects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81bd8eb",
   "metadata": {},
   "source": [
    "### Load Available Subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f88eb4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.load(\"/media/mohammad/New Volume/DoctoralSharif/Articles/RL and MDD/FittedData/AvailableSubjects.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f4fb89f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def custom_task_loader(file_path: str,available: NDArray = A[1,:], \n",
    "                      task_col: str = 'Task', subject_group:str = 'D',**kwargs) -> List[NDArray]:\n",
    "    \"\"\"\n",
    "    Custom data loader for CSV files with a Task column containing string lists of\n",
    "    [stimulus, reaction_time, choice, reward] arrays.\n",
    "    \n",
    "    Parameters:\n",
    "    - file_path (str): Path to the CSV file.\n",
    "    - subject_col (str): Column name for subject IDs (used for grouping).\n",
    "    - task_col (str): Column name for Task data (string lists of T*4 arrays).\n",
    "    - **kwargs: Additional arguments for pandas.read_csv.\n",
    "    \n",
    "    Returns:\n",
    "    - List[NDArray]: List of per-subject data arrays (shape (n_trials, 3)).\n",
    "      Each NDArray contains [stimulus, choice, reward] columns. The order of arrays\n",
    "      matches the order of subjects in the file.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path, **kwargs)\n",
    "    df = df.drop(37)\n",
    "\n",
    "    if subject_group == 'CTL':\n",
    "       df = df[df[\"BDI\"]<=7] \n",
    "    elif subject_group == \"D\":\n",
    "       df = df[df[\"BDI\"]>=13] \n",
    "\n",
    "    df = df[df[\"ID\"].isin(available)]\n",
    "       \n",
    "    \n",
    "    required_cols = [task_col]\n",
    "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
    "    \n",
    "    df = df.dropna(subset=required_cols)\n",
    "    if df.empty:\n",
    "        raise ValueError(\"No valid data after removing missing values\")\n",
    "    \n",
    "    behavioral_data: List[NDArray] = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        try:\n",
    "            task_data = np.array(literal_eval(row[task_col]), dtype=float)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to parse Task data for subject {row[task_col]}: {e}\")\n",
    "        \n",
    "        if task_data.ndim != 2 or task_data.shape[1] != 4:\n",
    "            raise ValueError(f\"Task data for subject {row[task_col]} must be T*4, got shape {task_data.shape}\")\n",
    "        \n",
    "        # Extract [stimulus, choice, reward] (indices 0, 2, 3)\n",
    "        data = task_data[:, [0, 2, 3]].astype(int)\n",
    "        behavioral_data.append(data)\n",
    "    \n",
    "    if not behavioral_data:\n",
    "        raise ValueError(\"No valid subject data found\")\n",
    "    \n",
    "    return behavioral_data\n",
    "\n",
    "len(custom_task_loader(PathTofile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05598411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup environment\n",
    "config_path = \"cognitive_models/tasks/mdp_pl_config.json\"  # Adjust path as needed\n",
    "try:\n",
    "    CONTEXTS, TRANSITIONS = parse_mdp_config(config_path)\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(f\"Configuration file not found: {config_path}\")\n",
    "\n",
    "class StateTask(State):\n",
    "    def __init__(self, name: str=None):\n",
    "        super().__init__(name)\n",
    "class StateQ(State):\n",
    "    def __init__(self, name: str=None, Q: NDArray[np.float64]=None):\n",
    "        super().__init__(name)\n",
    "        self.Q = Q\n",
    "\n",
    "\n",
    "# Create task and agent states(task state are real states and agents states are the representation of task states in agent)\n",
    "# Here agent states has additional Q attribute containing the value of each action potential for that states.\n",
    "StatesTask = np.array([State(name=k) for k in CONTEXTS.keys()])\n",
    "task_mdp = PL(StatesTask, CONTEXTS, TRANSITIONS)\n",
    "StatesAgent = np.array([StateQ(k, np.zeros((2,))) for k in CONTEXTS.keys()])\n",
    "\n",
    "# Initialize agent and environment\n",
    "agent = QL1_RL(task_mdp, StatesAgent, alphaP=0.1, alphaN=0.1)\n",
    "env = MDPEnvironment(agent, task_mdp)\n",
    "\n",
    "# Initialize EMGuassian with custom data loader\n",
    "em = EMGuassian(\n",
    "    environment=env,\n",
    "    num_params=2,  # Assuming 2 parameters (e.g., alpha, gamma)\n",
    "    num_iteration_em=20,  # Small number for testing\n",
    "    num_iteration_gradient_descent=10000,  # Reduced for faster testing\n",
    "    learning_rate=0.01,\n",
    "    tol=1e-6,\n",
    "    data_loader=custom_task_loader)\n",
    "\n",
    "try:\n",
    "    EstimatedData = em.fit(\n",
    "            behavioral_data=PathTofile,\n",
    "            task_col=\"Task\"\n",
    "        )\n",
    "except Exception as e:\n",
    "        print(f\"Model fitting failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1928d7",
   "metadata": {},
   "source": [
    "### Save Estimate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724cf3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"/media/mohammad/New Volume/DoctoralSharif/Articles/RL and MDD/FittedData/available_dp_data.pkl\", \"wb\") as file:\n",
    "    pickle.dump(EstimatedData, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ns-vqa1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
